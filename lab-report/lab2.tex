\section{Lab 2: Poisson's equation}

% One can thus implement the different schemes like red-black Gauss-Seidel iteration. This leads to
% a parallel program. However, there are numerous questions or problems that are worth to investigate.
% For instance, is it worth to make more than 1 iteration in each domain between two communication
% steps? The convergence of the algorithm will be slower if measured in the number of iteration steps,
% but if measured in real time it may or may not be advantageous. How much data should one transfer
% in a communication step? Just the data along the border, or maybe more layers at once? What is
% the best way to partition the grid points in to domains? Should these domains be as close to a square
% as possible, horizontal strips or vertical strips? How does the performance scale with problem size, or
% with increasing number of processes?

\subsection{Part 1}
\subsubsection{Step 1}

It is simple to understand that the program was indeed executed twice since two pairs of statements are written to the terminal in comparison to one pair before the modifications. Since we are now running the same program in two different nodes this behavior is expected.  

The result is the following
\begin{lstlisting}
    Number of iterations  : 2355
    Elapsed processortime : 1.350000 s
    Number of iterations  : 2355
    Elapsed processortime : 1.360000 s
\end{lstlisting}

\subsubsection{Step 2}

After adding the global variable and the necessary call to \texttt{MPI\_Comm\_rank} using the predefined communicator \texttt{MPI\_COMM\_WORLD}.
\begin{lstlisting}
    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
\end{lstlisting}

The following is printed to the standard output:
\begin{lstlisting}
    (0)     Elapsed processortime : 1.360000 s
    (0)     Number of iterations  : 2355
    (1)     Elapsed processortime : 1.360000 s
    (1)     Number of iterations  : 2355
\end{lstlisting}

\subsubsection{Step 3}

% After rewriting the timing functions mentioned in the exercise description, the new output is as follows:
% \begin{lstlisting}
%     (0)     Elapsed Wtime        : 1.410156 s ( 97.2% CPU)
%     (0)     Number of iterations : 2355
%     (1)     Elapsed Wtime        : 1.468750 s ( 94.0% CPU)
%     (1)     Number of iterations : 2355
% \end{lstlisting}

\subsubsection{Step 4}

Adjusting the code so each process writes to a separate file does not affect the text displayed, so there is no need to repeat it here. In addition by executing the command

\begin{lstlisting}
    diff output0.dat output1.dat
\end{lstlisting}

I was able to confirm the files are indeed identical.

\subsubsection{Step 5}

On this step, responsible to ensure correct distribution of information originated from an input file, several statements had to be rewritten. Below is a summary of those changes, in particular the parts that were not completely specified in the exercise manual.

To ensure only process 0 opens the file a simple comparison suffices
\begin{lstlisting}
    /* only process 0 may execute this if */
    if (proc_rank == 0)
    { ... }
\end{lstlisting}

To broadcast the data read from the file it is first necessary to explain which fields the \texttt{MPI\_Bcast(void *buffer, int count, MPI\_Datatype datatype, int root, MPI\_Comm comm))} function requires. 

For our situation the \texttt{buffer} pointer should refer to the address of the variable we want to broadcast. The \texttt{count} relates to the number of entries in the buffer. The \texttt{datatype} should describe the type of data the buffer points to, \eg for integers this should be \texttt{MPI\_INT}. The \texttt{root} is the message broadcaster, in our case node 0. Finally we will use the usual predefined communicator for the last argument \texttt{comm}.

Thus the broadcast calls are as follows
\begin{lstlisting}
    /* broadcast the array gridsize in one call */
    MPI_Bcast(&gridsize      , 2, MPI_INT   , 0, MPI_COMM_WORLD);
    /* broadcast precision_goal */
    MPI_Bcast(&precision_goal, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    /* broadcast max_iter */  
    MPI_Bcast(&max_iter      , 1, MPI_INT   , 0, MPI_COMM_WORLD);
(...)
    /* The return value of this scan is broadcast even though it is no input data */
    MPI_Bcast(&s, 1, MPI_INT, 0, MPI_COMM_WORLD);
(...)
    /* broadcast source_x */
    MPI_Bcast(&source_x  , 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    /* broadcast source_y */
    MPI_Bcast(&source_y  , 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    /* broadcast source_val */
    MPI_Bcast(&source_val, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);
\end{lstlisting}

\subsubsection{Step 6}

Following the same approach as in the previous section, only the finished version of incomplete code from the manual will be shown in the excerpt.
\begin{lstlisting}
    MPI_Comm_size(MPI_COMM_WORLD, &P);
    (...)
    MPI_Cart_create(MPI_COMM_WORLD, 2, P_grid, wrap_around, reorder, &grid_comm);
    (...)
    /* Rank of process in new communicator */
    MPI_Comm_rank(grid_comm, &proc_rank);
    /* Coordinates of process in new communicator */
    MPI_Cart_coords(grid_comm, proc_rank, 2, proc_coord);
    (...)
    /* rank of processes proc_top and proc_bottom */
    MPI_Cart_shift(grid_comm, Y_DIR, 1, &proc_top, &proc_bottom);
    /* rank of processes proc_left and proc_right */
    MPI_Cart_shift(grid_comm, X_DIR, 1, &proc_left, &proc_right);
\end{lstlisting}

There a couple new function calls on this code whose arguments I will explain next.
As explained in the exercise description MPI uses the \texttt{MPI\_Cart\_*} function calls to arrange tasks in a virtual process grid.

To create one the API calls needs the previous communicator, in our case we were using \texttt{MPI\_COMM\_WORLD}. \texttt{ndims} and \texttt{dims} define the number of dimensions of the grid and the number of processors in each, respectively. For our example these are 2 and \texttt{P\_grid}. Thereafter the periods specifies if the grid is periodic or not per dimension, and finally if the ranking is reordered or not. These are replaced by the self-explanatory variables \texttt{wrap\_around} and \texttt{reorder}. The new communicator is stored in the address of \texttt{comm\_cart}. From now on all pointers to communicators refer to this one.

\begin{lstlisting}
int MPI_Cart_create(MPI_Comm comm_old, int ndims, int *dims, int *periods, int reorder, MPI_Comm *comm_cart)
int MPI_Cart_coords(MPI_Comm comm, int rank, int maxdims, int *coords)
int MPI_Cart_shift(MPI_Comm comm, int direction, int displ, int *source, int *dest)
\end{lstlisting}

To define the coordinates of the process in the new communicator we use \texttt{MPI\_Cart\_coords}. Here the rank is necessarily the processor rank (\texttt{proc\_rank}), the maximum dimensions is 2. The coordinates of each specified process are stored in the \texttt{proc\_coord} array.

Finally the shift operation returns the shifted source and destination ranks. The direction and displacement quantity arguments are self-evident and are replaced in the program by \texttt{X\_DIR} or \texttt{Y\_DIR} and 1 respectively for horizontal and vertical displacements. In accordance to the outputs source and destination are stored in \texttt{proc\_top} and \texttt{proc\_bottom} or \texttt{proc\_left} and \texttt{proc\_right} depending on the direction. 


The text written to the standard output now features the coordinate of each processor.

\begin{lstlisting}
    (0) Number of iterations : 2355
    (0) (x,y)=(0,0)
    (0) Elapsed Wtime:       1.414062 s ( 96.9% CPU)
    (1) Number of iterations : 2355
    (1) (x,y)=(1,0)
    (1) Elapsed Wtime:       1.464844 s ( 95.6% CPU)
\end{lstlisting}

As one can observe since there are two processor, number 0 is allocated the left half of the grid, and processor with rank 1 deals with the right half.

\subsubsection{Step 7}
% After adjusting the \texttt{Setup\_Grid()} function as described the following results were obtained for three different setups. The first has two processors, the second three and the third four. The results can be seen below.

% \begin{lstlisting}
    % (0)   Number of iterations : 1195
    % (0) (x,y)=(0,0)
    % (0) Elapsed Wtime:       0.394531 s ( 86.2% CPU)
    % (1)   Number of iterations : 945
    % (1) (x,y)=(1,0)
    % (1) Elapsed Wtime:       0.347656 s ( 83.4% CPU)

    % (0)   Number of iterations : 1
    % (0) (x,y)=(0,0)
    % (0) Elapsed Wtime:       0.007812 s (  0.0% CPU)
    % (1)   Number of iterations : 695
    % (1) (x,y)=(1,0)
    % (1) Elapsed Wtime:       0.214844 s ( 79.1% CPU)
    % (2)   Number of iterations : 1
    % (2) (x,y)=(2,0)
    % (2) Elapsed Wtime:       0.046875 s (106.7% CPU)

    % (0)   Number of iterations : 791
    % (0) (x,y)=(0,0)
    % (0) Elapsed Wtime:       0.128906 s ( 85.3% CPU)
    % (1)   Number of iterations : 792
    % (1) (x,y)=(0,1)
    % (1) Elapsed Wtime:       0.160156 s ( 99.9% CPU)
    % (2)   Number of iterations : 1
    % (2) (x,y)=(1,0)
    % (2) Elapsed Wtime:       0.058594 s ( 68.3% CPU)
    % (3)   Number of iterations : 791
    % (3) (x,y)=(1,1)
    % (3) Elapsed Wtime:       0.164062 s ( 97.5% CPU)
% \end{lstlisting}

When there are three processors the work can not be evenly split between them. This can be confirmed by inspecting the \texttt{x} and \texttt{y} variables in the \texttt{Setup\_grid function}.

For example for processor 2 (in a 3 processor configuration) it is visible that \texttt{x} is always negative.

\begin{lstlisting}
    (2) x = -30, dim[X_DIR] = 36
    (2) y =  71,  dim[Y_DIR] = 102

    (2) x = -3, dim[X_DIR] = 36
    (2) y = 76, dim[Y_DIR] = 102

    (2) x = -28, dim[X_DIR] = 36
    (2) y =  26, dim[Y_DIR] = 102
\end{lstlisting}


\subsubsection{Step 8}
\subsubsection{Step 9}

After implementing the collective reduction operation the total number of iterations is indeed the same, as confirmed by the program's output.

\begin{lstlisting}
    (0) Number of iterations : 2355
    (0) Elapsed Wtime:       1.507812 s ( 96.2% CPU)
    (1) Number of iterations : 2355
    (1) Elapsed Wtime:       1.496094 s ( 97.6% CPU)
    (2) Number of iterations : 2355
    (2) Elapsed Wtime:       1.507812 s ( 96.8% CPU)
\end{lstlisting}

\subsubsection{Step 10}


\subsection{Part 2}

\subsubsection{2.1}
\subsubsection{2.2}

After changing the code to accommodate for the algorithmic improvement, 5 tests were performed to compare different values for the relaxation parameter $\omega$. The results are summarized in table~\ref{tbl:omega}.
From this data we concluded that 1.93 is the optimal value for the relaxation parameter, accomplishing almost 18 times less iterations than the original with $\omega$ equal to one.

\begin{table}[H]
\centering
\begin{tabular}{cccc}
 \toprule
$\omega$ & Maximum \texttt{Wtime}\tablefootnote{The maximum time was computed over the four individual processor times for each $\omega$ value.} (\si{s}) & Iterations & Reduction\\ \midrule
    1.00 &        1.500000 &        2355 &        1.00 \\    
    1.90 &        0.222656 &        220  &        10.7 \\
    1.92 &        0.199219 &        165  &        14.3 \\
\bf 1.93 &    \bf 0.175781 &    \bf 131  &    \bf 18.0 \\
    1.94 &        0.289062 &        142  &        16.6 \\
    1.98 &        0.351562 &        419  &        5.62 \\
\bottomrule
\end{tabular}
\caption{Time, number of iterations obtained and respective iteration reduction for different $\omega$ values. The topology used was \texttt{pt:~4~4~1} with a grid size of \texttt{g:~100~x~100}.}
\label{tbl:omega}
\end{table}


\subsubsection{2.3}

The goal of this exercise is to investigate the scaling behavior of the code with a fixed relaxation parameter. To accomplish that analysis several runs were measured with various grid sizes. In addition different \emph{slices} were also tested.

\begin{table}[H]
\centering
\begin{tabular}{cccc}
 \toprule
Grid Size & \texttt{Wtime} for 4~4~1 (\si{s}) & \texttt{Wtime} for 4~2~2 (\si{s}) & $\Delta$ (\%)\\ \midrule
200~x~200   &     0.464844 &     0.699219 &    50.42 \\
400~x~400   &     1.054688 &     1.054688 &     0.00 \\
800~x~800   &     3.742188 &     3.726562 &    -0.42 \\
2000~x~2000 &    21.351562 &    21.328125 &    -0.11 \\
\bottomrule
\end{tabular}
\caption{The maximum time for different grid sizes and different \emph{slicing} arrangements. The $\Delta$ column compares the relative difference in performance between the two previous columns. The number of iterations for each run was fixed at 300.}
\label{tbl:slices}
\end{table}

The results of the aforementioned experience are shown in table~\ref{tbl:slices}. As one can see as the grid size increase the difference in the different topologies become negligible. Nonetheless for small sizes the 4~4~1 topology is significantly faster.

To to determine the average time per iteration the time will be parametrized as follows
\[
t(n) = \alpha + \beta \cdot n
\]

Where $\alpha$ and $\beta$ are arbitrary constants.

\begin{table}[H]
\centering
\begin{tabular}{cccc}
 \toprule
Grid Size & $\alpha$ & $\beta$ \\ \midrule
200~x~200   &     0.464844 &     0.699219 &    50.42 \\
400~x~400   &     1.054688 &     1.054688 &     0.00 \\
800~x~800   &     3.742188 &     3.726562 &    -0.42 \\
2000~x~2000 &    21.351562 &    21.328125 &    -0.11 \\
\bottomrule
\end{tabular}
\caption{The maximum time for different grid sizes and different \emph{slicing} arrangements. The $\Delta$ column compares the relative difference in performance between the two previous columns. The number of iterations for each run was fixed at 300.}
\label{tbl:alphabeta}
\end{table}