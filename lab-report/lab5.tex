\section{Lab 5: Matmul}

This section is dedicated to the implementation of the general matrix multiplication (GEMM). Equation~\ref{eq:mmul} describes the operation.
$A$, $B$ and $C$ are the matrices to be multiplied and the result respectively, while $\alpha$ and $\beta$ are scalar coefficients. 
The developed code was built upon the provided sequential implementation (\texttt{matmul.c}).
It uses the Message Passing Interface (MPI) for inter-node communication and Open Multi-Processing (OpenMP) for intra-node computations.

Unless specifically specified the code is running on 4 nodes from DAS3. This assumption facilitates the explanation of the proposed solution.
Nonetheless the C code implementation is capable of handle an arbitrary number of nodes as long as the grid can be evenly distributed among them.
Finally the compiler used was \texttt{gcc 4.3.2} invoked with several optimization flags that shall be enumerated in~\ref{ssec:results}.


\begin{equation}
C = \alpha AB + \beta C
\label{eq:mmul}
\end{equation}

This section is organized as follows: in subsection~\ref{ssec:arch} the higher level architecture of the code is explained.
In essence it covers how the matrices are partitioned and divided among processors to enable parallel computation and what communication steps are necessary top obtain the final result.
Subsequently subsection~\ref{ssec:results} will be focused on the performance analysis of this implementation.
The work presented in this document was inspired by several sources in addition to the course materials, namely \cite{sandeep, ortega, chien}.

\subsection{Architecture}
\label{ssec:arch}

The first step is to divide the matrices evenly among the 4 nodes available.
The subscripts in equations \ref{eq:partition}-\ref{eq:partition2} indicate the node responsible for the subset. 

\begin{equation}
C = \alpha AB + \beta C
\label{eq:mainproblem}
\end{equation}

\begin{align}
    \begin{bmatrix}
      C_0 & C_1 \\
      C_2 & C_3 \\
    \end{bmatrix}
= \alpha
    \begin{bmatrix}
      A_0 & A_1 \\
      A_2 & A_3 \\
    \end{bmatrix}
\cdot
    \begin{bmatrix}
      B_0 & B_1 \\
      B_2 & B_3 \\
    \end{bmatrix}
+ \beta
    \begin{bmatrix}
      C_0 & C_1 \\
      C_2 & C_3 \\
    \end{bmatrix}
\label{eq:partition}
\end{align}

\begin{align}
C_0 &= \alpha (A_0 B_0 + A_1 B_2) + \beta C_0\\
C_1 &= \alpha (A_0 B_1 + A_1 B_3) + \beta C_1\\
C_2 &= \alpha (A_2 B_0 + A_3 B_2) + \beta C_2\\
C_3 &= \alpha (A_2 B_1 + A_3 B_3) + \beta C_3
\label{eq:partition2}
\end{align}

Therefore each node requires two parts from each of A and B to calculate its share of C. Moreover it is not surprising to find that the way A and B are divided is different, while A is partitioned horizontally, B is sliced vertically. For simplicity purposes, in the actual implementation each node initializes the matrices independently but coherently although it only uses the subsets it needs. Alternatively this could be done using MPI. For example node zero would initialize all matrices and send the respective parts to the remaining three nodes.

The next step involves three operations per node: two multiplications and one addition. The multiplication operation is realized in a similar fashion to the method demonstrated in~\cite{chien}. The main idea is to divide each matrix (or part of a matrix in this case) in smaller blocks of size \texttt{BLOCK\_SIZE} to make better use of the cache space. In addition OpenMP directives are used to parallelize this work. The implementation includes a series of \texttt{for} loops that were suitable candidates for these directives. After various experiments we elected, based on the performance metrics, to simply parallelize the outer loop.

The following code excerpt illustrates how to invoke the directive.
\begin{lstlisting}
#pragma omp parallel for default(none) private(...)
  shared(...)
\end{lstlisting}

Other options for parallelizing the loop, shown below, were studied but with less interesting results.

\begin{lstlisting}
#pragma omp for schedule(dynamic) nowait
#pragma omp for schedule(dynamic, chunk) nowait
#pragma omp for schedule(runtime) nowait
\end{lstlisting}

Another parameter that needed to be selected was the \texttt{BLOCK\_SIZE}. Once again empirical data was the method used to select the best data. This information can be observed in table~\ref{tbl:omp}. Based on it the value 36 was chosen. The remainder of the data included in this document takes this selection into account.


\begin{table}[H]
\centering
\begin{tabular}{*{3}{c}}
 \toprule
\texttt{BLOCK\_SIZE} &   Total time (\si{s}) &  Maximum GFLOPs   \\ \midrule
16                   &   0.3772              &  2.8935           \\
32                   &   0.3475              &  3.9204           \\
\bf 36               &   \bf 0.3290          &  \bf 4.6831       \\
48                   &   0.3344              &  4.5406           \\
64                   &   0.3380              &  4.3492           \\
128                  &   0.5975              &  1.5966           \\
\bottomrule
\end{tabular}
\caption{Determining the best \texttt{BLOCK\_SIZE} for a 2048x2048 grid.}
\label{tbl:omp}
\end{table}


The major final step is to aggregate all values back into a single node. This is accomplished through calls to \texttt{MPI\_Send()} and \texttt{MPI\_Recv()}.
Node zero was elected to collect the data from the remaining nodes. Thereafter it compiles all of it, including its own share, in a single 2D array and writes the result to a text file.

In summary three main steps were necessary to solve equation~\ref{eq:mainproblem}:
\begin{enumerate}
\item Divide evenly the input matrices A and B between nodes.
\item Compute the required operations over the node-restricted subsets.
\item Aggregate all results in a single node and write results to a file.
\end{enumerate}

Although the high level overview is strikingly simple the implementation was not so straightforward. As always the devil is in the details and in particular properly handling the indexes of each matrix and submatrix in an abstract way proved to be quite cumbersome.

The last remark of this section is dedicated to the compiler flags used, both for functional and performance purposes.
In order to generate a valid executable three flags were use:
\begin{enumerate}
\item \texttt{-lrt}: links the real time libraries necessary for the timing operations.
\item \texttt{-lm}: enable the operations enumerated in \texttt{math.h}
\item \texttt{-Wall}: shows all warnings at compile time. Used solely for debugging purposes.
\end{enumerate}

For optimization purposes several options were explored from the vast array of options provided by \texttt{gcc 4.3.2}. For more information see \cite{gcc}.

\begin{enumerate}
% -O2 -ffast-math -march=native -mmmx -m3dnow -funroll-loops
\item \texttt{-O2}: GCC performs nearly all supported optimizations that do not involve a space-speed tradeoff.
\item \texttt{-ffast-math}: attempts to improve the performance of math operations. It may break some implementations as it may not follow the exact implementation of IEEE or ISO rules/specifications for math functions.
\item \texttt{-march=native}, \texttt{-mmmx} and  \texttt{-m3dnow} : makes use of target-specific extensions of the instruction set.
\item \texttt{-funroll-loop}: as the name implies it unrolls loops. This flag increases the code size and may or may not results in a shorter execution speed.
\end{enumerate}

\subsection{Results}
\label{ssec:results}

In the text that follows the results of the previously described implementation will be presented and analyzed.

Table~\ref{tbl:results} includes the results for the implementation of the problem defined in equation~\ref{eq:mainproblem} using the approach outlined in section~\ref{ssec:arch}.
The metrics presented are essentially the same as included with \texttt{matmul.c}, provided with the lab assignment, which implements a sequential solution to the same problem.

\begin{table}[H]
\centering
\begin{tabular}{*{5}{c}}
 \toprule
Dimension &  Rank &  Total time (\si{s}) &  GFLOPs    &  Bandwidth (\si[per-mode=symbol]{\byte\per\nano\second}) \\ \midrule
\multirow{4}{*}{1024}
&  0  &  0.634124  &  3.386536  &  0.052915 \\
&  1  &  0.496178  &  4.328052  &  0.067626 \\
&  2  &  0.527540  &  4.070751  &  0.063605 \\
&  3  &  0.622204  &  3.451413  &  0.053928 \\\\
%
\multirow{4}{*}{2048}
&  0 &   4.311446  &   3.984712  &  0.031131 \\
&  1 &   3.807184  &   4.512487  &  0.035254 \\
&  2 &   3.885920  &   4.421056  &  0.034539 \\
&  3 &   4.272333  &   4.021191  &  0.031416 \\\\
%
\multirow{4}{*}{4096}
&  0 &   32.34151  &   4.249614  &  0.016600 \\
&  1 &   30.26010  &   4.541919  &  0.017742 \\
&  2 &   30.65119  &   4.483967  &  0.017515 \\
&  3 &   32.17709  &   4.271329  &  0.016685 \\
\bottomrule
\end{tabular}
\caption{Performance metrics obtained for the implementation of the General Matrix Multiplication.}
\label{tbl:results}
\end{table}

The overall performance increases slightly with the dimensions. The peak GLOPs was measured above to \emph{4.5}.

The theoretical GFLOP is calculated as shown in equation~\ref{eq:flop}.

\begin{align}
\text{FLOPs}   &=~\text{Core Count} \cdot \text{Core Frequency} \cdot \frac{\text{FLOPs}}{\text{cycle}} \\
\text{GFLOPs\footnotemark} &=~2 \cdot 2.612 \cdot 4 \approx 21
\label{eq:flop}
\end{align}
\footnotetext{The values used were obtained through inspection of the \texttt{/proc/cpuinfo} file.}

Therefore the current implementation is able to operate at around 20\% of the theoretical FLOP capacity.


To better understand where most of the time is spent more fine-grained time measurements were performed.
In table~\ref{tbl:timing} we discriminate between communication times and CPU times.
The former considers the time consumed in the code that includes \texttt{MPI\_Send} and \texttt{MPI\_Recv} function calls.
While the latter relates to the time necessary to complete the multiplication operations.
As previous explained these operations are carried out by the individual nodes over subsets of the input matrices.

\begin{table}[H]
\centering
\begin{tabular}{*{5}{c}}
 \toprule
Dimension & Rank  &  Total time (\si{s})  &  Comm time  (\si{s})  &  CPU time(\si{s})  \\ \midrule 
\multirow{4}{*}{1024}
&  0  &  0.634124  &  0.222224  &  0.411894   \\
&  1  &  0.496178  &  0.082787  &  0.413384   \\
&  2  &  0.527540  &  0.114259  &  0.413273   \\
&  3  &  0.622204  &  0.208747  &  0.413450   \\\\
\multirow{4}{*}{2048}
&  0  &  4.311446  &  0.863112  &  3.448329   \\
&  1  &  3.807184  &  0.329370  &  3.477807   \\
&  2  &  3.885920  &  0.427981  &  3.457932   \\
&  3  &  4.272333  &  0.784389  &  3.487938   \\\\
\multirow{4}{*}{4096}
&  0  &  32.341511  &  3.418472  &  28.923031   \\
&  1  &  30.260109  &  2.407693  &  27.852407   \\
&  2  &  30.651197  &  2.558106  &  28.093084   \\
&  3  &  32.177096  &  3.502032  &  28.675056   \\

\bottomrule
\end{tabular}
\caption{Empirical data with the communication and CPU times discriminated.}
\label{tbl:timing}
\end{table}


The conclusion drawn from table~\ref{tbl:timing} is evident: the majority of time (around $90\%$) is dedicated to the actual multiplications.
This is not surprising as there are only four nodes and each communicates just once with the root node (\ie node with rank zero).
        





